{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d12fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a dark background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9fecfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in your csv file that has the mid features. \n",
    "\n",
    "#data = pd.read_csv('../Feature Extraction/midFeaturesTrainSet.csv')\n",
    "data = pd.read_csv('../Data/Mid_features/')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test set, stratified by Emotion.\n",
    "data_train, data_test = train_test_split(data.copy(),\n",
    "                                   shuffle=True,\n",
    "                                   random_state=608,\n",
    "                                   stratify=data.Emotion,\n",
    "                                   test_size=0.2\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e4f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ae0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the percentages of the different emotion categories in the training set\n",
    "\n",
    "data_train.Emotion.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eedfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the percentages of the different emotion categories in the test set\n",
    "\n",
    "data_test.Emotion.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b917cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[['Emotion']]\n",
    "y_test  = data_test[['Emotion']]\n",
    "\n",
    "X_train = data_train.drop(columns  = ['FileID','actorID', 'Emotion', 'SentenceID'])\n",
    "X_test  = data_test.drop(columns   = ['FileID','actorID', 'Emotion', 'SentenceID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6115a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to the y vectors encoding each of the emotions.\n",
    "\n",
    "y_train_dummies = pd.get_dummies(y_train)\n",
    "y_train         = pd.concat([y_train, y_train_dummies], axis=1)\n",
    "y_train[\"Emotion_ALL\"]  = 1*y_train[\"Emotion_NEU\"] + 2*y_train[\"Emotion_ANG\"] + 3*y_train[\"Emotion_HAP\"] + 4*y_train[\"Emotion_SAD\"] + 5*y_train[\"Emotion_FEA\"] + 6*y_train[\"Emotion_DIS\"]\n",
    "y_train = y_train.drop(columns  = [\"Emotion\", \"Emotion_NEU\", \"Emotion_ANG\", \"Emotion_HAP\", \"Emotion_SAD\", \"Emotion_FEA\", \"Emotion_DIS\"])\n",
    "\n",
    "y_test_dummies = pd.get_dummies(y_test)\n",
    "y_test         = pd.concat([y_test, y_test_dummies], axis=1)\n",
    "y_test[\"Emotion_ALL\"]  = 1*y_test[\"Emotion_NEU\"] + 2*y_test[\"Emotion_ANG\"] + 3*y_test[\"Emotion_HAP\"] + 4*y_test[\"Emotion_SAD\"] + 5*y_test[\"Emotion_FEA\"] + 6*y_test[\"Emotion_DIS\"]\n",
    "y_test  = y_test.drop(columns   = [\"Emotion\", \"Emotion_NEU\", \"Emotion_ANG\", \"Emotion_HAP\", \"Emotion_SAD\", \"Emotion_FEA\", \"Emotion_DIS\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline to first scale the mid feature data, then apply the SVC\n",
    "\n",
    "pipe100 = Pipeline([('scale', StandardScaler()),\n",
    "                 ('svc', SVC(kernel='rbf'))])\n",
    "\n",
    "classifier100 = pipe100.fit(X_train, y_train)\n",
    "pred100       = pipe100.predict(X_test)    \n",
    "\n",
    "# Look at the confusion matrix for the test data :\n",
    "cnf_matrix_test100 = confusion_matrix(y_test, pred100)\n",
    "\n",
    "print(\"no PCA\")\n",
    "print(\"confusion matrix for all six emotions of the test set is:\")\n",
    "print(cnf_matrix_test100)\n",
    "print()\n",
    "\n",
    "# Look at the confusion matrix for the training data:\n",
    "pred_train100       = pipe100.predict(X_train)\n",
    "cnf_matrix_train100 = confusion_matrix(y_train, pred_train100)\n",
    "\n",
    "print(\"confusion matrix for all six emotions of the train set is:\")\n",
    "print(cnf_matrix_train100)\n",
    "\n",
    "class_names = [\"NEU\", \"ANG\", \"HAP\", \"SAD\", \"FEA\", \"DIS\"]\n",
    "disp = plot_confusion_matrix(classifier100, X_test, y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "acc100 = np.diag(cnf_matrix_test100).sum()/cnf_matrix_test100.sum() \n",
    "print(\"accuracy =\", acc100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb5306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new95 = X_train.copy()\n",
    "X_test_new95  = X_test.copy()\n",
    "\n",
    "pca95 = PCA(n_components = .95)\n",
    "pca95.fit(X_train)\n",
    "\n",
    "X_train_transform95 = pca95.transform(X_train_new95)\n",
    "X_test_transform95  = pca95.transform(X_test_new95)\n",
    "\n",
    "print(len(X_train_transform95[0]))\n",
    "\n",
    "for i in range (len(X_train_transform95[0])):\n",
    "    X_train_new95[\"comp_\" + str(i+1)] = X_train_transform95[:,i]\n",
    "    X_test_new95 [\"comp_\" + str(i+1)] = X_test_transform95[:,i]\n",
    "\n",
    "X_train_sub95  = X_train_new95.iloc[: , -(len(X_train_transform95[0])+1):]\n",
    "X_test_sub95   = X_test_new95.iloc[: , -(len(X_test_transform95[0])+1):]    \n",
    "\n",
    "# Build pipeline to first scale the mid feature data, then apply the SVC\n",
    "\n",
    "pipe95 = Pipeline([('scale', StandardScaler()),\n",
    "                 ('svc', SVC(kernel='rbf'))])\n",
    "\n",
    "classifier95 = pipe95.fit(X_train_sub95, y_train)\n",
    "pred95       = pipe95.predict(X_test_sub95)    \n",
    "\n",
    "# Look at the confusion matrix for the test data :\n",
    "cnf_matrix_test95 = confusion_matrix(y_test, pred95)\n",
    "\n",
    "print(\"for PCA n_components = 0.95\")\n",
    "print(\"confusion matrix for all six emotions of the test set is:\")\n",
    "print(cnf_matrix_test95)\n",
    "print()\n",
    "\n",
    "# Look at the confusion matrix for the training data:\n",
    "pred_train95       = pipe95.predict(X_train_sub95)\n",
    "cnf_matrix_train95 = confusion_matrix(y_train, pred_train95)\n",
    "\n",
    "print(\"confusion matrix for all six emotions of the train set is:\")\n",
    "print(cnf_matrix_train95)\n",
    "\n",
    "class_names = [\"NEU\", \"ANG\", \"HAP\", \"SAD\", \"FEA\", \"DIS\"]\n",
    "disp = plot_confusion_matrix(classifier95, X_test_sub95, y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "acc95 = np.diag(cnf_matrix_test95).sum()/cnf_matrix_test95.sum() \n",
    "print(\"accuracy =\", acc95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfdd53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new90 = X_train.copy()\n",
    "X_test_new90  = X_test.copy()\n",
    "\n",
    "pca90 = PCA(n_components = .9)\n",
    "pca90.fit(X_train)\n",
    "\n",
    "X_train_transform90 = pca90.transform(X_train_new90)\n",
    "X_test_transform90  = pca90.transform(X_test_new90)\n",
    "\n",
    "print(len(X_train_transform90[0]))\n",
    "\n",
    "for i in range (len(X_train_transform90[0])):\n",
    "    X_train_new90[\"comp_\" + str(i+1)] = X_train_transform90[:,i]\n",
    "    X_test_new90 [\"comp_\" + str(i+1)] = X_test_transform90[:,i]\n",
    "\n",
    "X_train_sub90  = X_train_new90.iloc[: , -(len(X_train_transform90[0])+1):]\n",
    "X_test_sub90  = X_test_new90.iloc[: , -(len(X_test_transform90[0])+1):]    \n",
    "\n",
    "# Build pipeline to first scale the mid feature data, then apply the SVC\n",
    "\n",
    "pipe90 = Pipeline([('scale', StandardScaler()),\n",
    "                 ('svc', SVC(kernel='rbf'))])\n",
    "\n",
    "classifier90 = pipe90.fit(X_train_sub90, y_train)\n",
    "pred90       = pipe90.predict(X_test_sub90)    \n",
    "\n",
    "# Look at the confusion matrix for the test data :\n",
    "cnf_matrix_test90 = confusion_matrix(y_test, pred90)\n",
    "\n",
    "print(\"for PCA n_components = 0.9\")\n",
    "print(\"confusion matrix for all six emotions of the test set is:\")\n",
    "print(cnf_matrix_test90)\n",
    "print()\n",
    "\n",
    "# Look at the confusion matrix for the training data:\n",
    "pred_train90       = pipe90.predict(X_train_sub90)\n",
    "cnf_matrix_train90 = confusion_matrix(y_train, pred_train90)\n",
    "\n",
    "print(\"confusion matrix for all six emotions of the train set is:\")\n",
    "print(cnf_matrix_train90)\n",
    "\n",
    "class_names = [\"NEU\", \"ANG\", \"HAP\", \"SAD\", \"FEA\", \"DIS\"]\n",
    "disp = plot_confusion_matrix(classifier90, X_test_sub90, y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "acc90 = np.diag(cnf_matrix_test90).sum()/cnf_matrix_test90.sum() \n",
    "print(\"accuracy =\", acc90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e03e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new85 = X_train.copy()\n",
    "X_test_new85  = X_test.copy()\n",
    "\n",
    "pca85 = PCA(n_components = .85)\n",
    "pca85.fit(X_train)\n",
    "\n",
    "X_train_transform85 = pca85.transform(X_train_new85)\n",
    "X_test_transform85  = pca85.transform(X_test_new85)\n",
    "\n",
    "print(len(X_train_transform85[0]))\n",
    "\n",
    "for i in range (len(X_train_transform85[0])):\n",
    "    X_train_new85[\"comp_\" + str(i+1)] = X_train_transform85[:,i]\n",
    "    X_test_new85 [\"comp_\" + str(i+1)] = X_test_transform85[:,i]\n",
    "\n",
    "X_train_sub85  = X_train_new85.iloc[: , -(len(X_train_transform85[0])+1):]\n",
    "X_test_sub85  = X_test_new85.iloc[: , -(len(X_test_transform85[0])+1):]    \n",
    "\n",
    "# Build pipeline to first scale the mid feature data, then apply the SVC\n",
    "\n",
    "pipe85 = Pipeline([('scale', StandardScaler()),\n",
    "                 ('svc', SVC(kernel='rbf'))])\n",
    "\n",
    "classifier85 = pipe85.fit(X_train_sub85, y_train)\n",
    "pred85       = pipe85.predict(X_test_sub85)    \n",
    "\n",
    "# Look at the confusion matrix for the test data :\n",
    "cnf_matrix_test85 = confusion_matrix(y_test, pred85)\n",
    "\n",
    "print(\"for PCA n_components = 0.85\")\n",
    "print(\"confusion matrix for all six emotions of the test set is:\")\n",
    "print(cnf_matrix_test85)\n",
    "print()\n",
    "\n",
    "# Look at the confusion matrix for the training data:\n",
    "pred_train85       = pipe85.predict(X_train_sub85)\n",
    "cnf_matrix_train85 = confusion_matrix(y_train, pred_train85)\n",
    "\n",
    "print(\"confusion matrix for all six emotions of the train set is:\")\n",
    "print(cnf_matrix_train85)\n",
    "\n",
    "class_names = [\"NEU\", \"ANG\", \"HAP\", \"SAD\", \"FEA\", \"DIS\"]\n",
    "disp = plot_confusion_matrix(classifier85, X_test_sub85, y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "acc85 = np.diag(cnf_matrix_test85).sum()/cnf_matrix_test85.sum() \n",
    "print(\"accuracy =\", acc85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FP100 = cnf_matrix_test100.sum(axis=0) - np.diag(cnf_matrix_test100) \n",
    "FN100 = cnf_matrix_test100.sum(axis=1) - np.diag(cnf_matrix_test100)\n",
    "TP100 = np.diag(cnf_matrix_test100)\n",
    "TN100 = cnf_matrix_test100.sum() - (FP100 + FN100 + TP100)\n",
    "\n",
    "\n",
    "FP95 = cnf_matrix_test95.sum(axis=0) - np.diag(cnf_matrix_test95) \n",
    "FN95 = cnf_matrix_test95.sum(axis=1) - np.diag(cnf_matrix_test95)\n",
    "TP95 = np.diag(cnf_matrix_test95)\n",
    "TN95 = cnf_matrix_test95.sum() - (FP95 + FN95 + TP95)\n",
    "\n",
    "FP90 = cnf_matrix_test90.sum(axis=0) - np.diag(cnf_matrix_test90) \n",
    "FN90 = cnf_matrix_test90.sum(axis=1) - np.diag(cnf_matrix_test90)\n",
    "TP90 = np.diag(cnf_matrix_test90)\n",
    "TN90 = cnf_matrix_test90.sum() - (FP90 + FN90 + TP90)\n",
    "\n",
    "FP85 = cnf_matrix_test85.sum(axis=0) - np.diag(cnf_matrix_test85) \n",
    "FN85 = cnf_matrix_test85.sum(axis=1) - np.diag(cnf_matrix_test85)\n",
    "TP85 = np.diag(cnf_matrix_test85)\n",
    "TN85 = cnf_matrix_test85.sum() - (FP85 + FN85 + TP85)\n",
    "\n",
    "acc100 = np.diag(cnf_matrix_test100).sum()/cnf_matrix_test100.sum() \n",
    "acc95 = np.diag(cnf_matrix_test95).sum()/cnf_matrix_test95.sum() \n",
    "acc90 = np.diag(cnf_matrix_test90).sum()/cnf_matrix_test90.sum() \n",
    "acc85 = np.diag(cnf_matrix_test85).sum()/cnf_matrix_test85.sum() \n",
    "\n",
    "recall100 = TP100/(TP100 + FN100)\n",
    "recall95 = TP95/(TP95 + FN95)\n",
    "recall90 = TP90/(TP90 + FN90)\n",
    "recall85 = TP85/(TP85 + FN85)\n",
    "\n",
    "print(\"accuracy with 100% variance of data  =\", acc100)\n",
    "print(\"accuracy with 95% variance of data =\", acc95)\n",
    "print(\"accuracy with 90% variance of data =\", acc90)\n",
    "print(\"accuracy with 85% variance of data =\", acc85)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"recall with 100% variance of data  =\", recall100)\n",
    "print(\"recall with 95% variance of data =\",recall95)\n",
    "print(\"recall with 90% variance of data =\",recall90)\n",
    "print(\"recall with 85% variance of data =\",recall85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2893ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9addde40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b447d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
