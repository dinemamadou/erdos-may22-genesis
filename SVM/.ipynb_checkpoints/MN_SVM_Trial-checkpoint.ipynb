{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d12fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a dark background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9fecfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab69a78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4876, 140)\n"
     ]
    }
   ],
   "source": [
    "# Read in your csv file that has the mid features. \n",
    "\n",
    "#data = pd.read_csv('../Feature Extraction/midFeaturesTrainSet.csv')\n",
    "data = pd.read_csv('midFeaturesTrainFinal.csv')\n",
    "\n",
    "\n",
    "data.head()\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb50b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test set, stratified by Emotion.\n",
    "\n",
    "\n",
    "data_train, data_test = train_test_split(data.copy(),\n",
    "                                   shuffle=True,\n",
    "                                   random_state=608,\n",
    "                                   stratify=data.Emotion,\n",
    "                                   test_size=0.2\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25e4f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileID</th>\n",
       "      <th>actorID</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>SentenceID</th>\n",
       "      <th>zcr_mean</th>\n",
       "      <th>energy_mean</th>\n",
       "      <th>energy_entropy_mean</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_spread_mean</th>\n",
       "      <th>spectral_entropy_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>delta chroma_4_std</th>\n",
       "      <th>delta chroma_5_std</th>\n",
       "      <th>delta chroma_6_std</th>\n",
       "      <th>delta chroma_7_std</th>\n",
       "      <th>delta chroma_8_std</th>\n",
       "      <th>delta chroma_9_std</th>\n",
       "      <th>delta chroma_10_std</th>\n",
       "      <th>delta chroma_11_std</th>\n",
       "      <th>delta chroma_12_std</th>\n",
       "      <th>delta chroma_std_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>1015_TIE_HAP_XX</td>\n",
       "      <td>1015</td>\n",
       "      <td>HAP</td>\n",
       "      <td>TIE</td>\n",
       "      <td>0.102349</td>\n",
       "      <td>0.007928</td>\n",
       "      <td>2.945650</td>\n",
       "      <td>0.191851</td>\n",
       "      <td>0.201698</td>\n",
       "      <td>0.689319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019953</td>\n",
       "      <td>0.032110</td>\n",
       "      <td>0.032458</td>\n",
       "      <td>0.031829</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>0.013863</td>\n",
       "      <td>0.024027</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.010913</td>\n",
       "      <td>0.011224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>1012_ITS_ANG_XX</td>\n",
       "      <td>1012</td>\n",
       "      <td>ANG</td>\n",
       "      <td>ITS</td>\n",
       "      <td>0.156747</td>\n",
       "      <td>0.013286</td>\n",
       "      <td>2.967563</td>\n",
       "      <td>0.240208</td>\n",
       "      <td>0.213694</td>\n",
       "      <td>0.823183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020356</td>\n",
       "      <td>0.029753</td>\n",
       "      <td>0.027887</td>\n",
       "      <td>0.016530</td>\n",
       "      <td>0.024693</td>\n",
       "      <td>0.022607</td>\n",
       "      <td>0.006172</td>\n",
       "      <td>0.008348</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.009163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>1025_IOM_NEU_XX</td>\n",
       "      <td>1025</td>\n",
       "      <td>NEU</td>\n",
       "      <td>IOM</td>\n",
       "      <td>0.101465</td>\n",
       "      <td>0.025004</td>\n",
       "      <td>2.918938</td>\n",
       "      <td>0.194020</td>\n",
       "      <td>0.206082</td>\n",
       "      <td>0.339042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.022066</td>\n",
       "      <td>0.056758</td>\n",
       "      <td>0.029304</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.033748</td>\n",
       "      <td>0.030512</td>\n",
       "      <td>0.029402</td>\n",
       "      <td>0.022541</td>\n",
       "      <td>0.014442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>1073_WSI_ANG_XX</td>\n",
       "      <td>1073</td>\n",
       "      <td>ANG</td>\n",
       "      <td>WSI</td>\n",
       "      <td>0.138922</td>\n",
       "      <td>0.008128</td>\n",
       "      <td>2.968660</td>\n",
       "      <td>0.228308</td>\n",
       "      <td>0.200131</td>\n",
       "      <td>0.996025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>0.013259</td>\n",
       "      <td>0.016366</td>\n",
       "      <td>0.011028</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.012716</td>\n",
       "      <td>0.022112</td>\n",
       "      <td>0.025750</td>\n",
       "      <td>0.009388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412</th>\n",
       "      <td>1065_IOM_DIS_XX</td>\n",
       "      <td>1065</td>\n",
       "      <td>DIS</td>\n",
       "      <td>IOM</td>\n",
       "      <td>0.069997</td>\n",
       "      <td>0.022606</td>\n",
       "      <td>3.006345</td>\n",
       "      <td>0.160496</td>\n",
       "      <td>0.182482</td>\n",
       "      <td>0.408786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035430</td>\n",
       "      <td>0.013289</td>\n",
       "      <td>0.015212</td>\n",
       "      <td>0.015347</td>\n",
       "      <td>0.005723</td>\n",
       "      <td>0.018372</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.014215</td>\n",
       "      <td>0.010749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               FileID  actorID Emotion SentenceID  zcr_mean  energy_mean  \\\n",
       "782   1015_TIE_HAP_XX     1015     HAP        TIE  0.102349     0.007928   \n",
       "615   1012_ITS_ANG_XX     1012     ANG        ITS  0.156747     0.013286   \n",
       "1405  1025_IOM_NEU_XX     1025     NEU        IOM  0.101465     0.025004   \n",
       "3800  1073_WSI_ANG_XX     1073     ANG        WSI  0.138922     0.008128   \n",
       "3412  1065_IOM_DIS_XX     1065     DIS        IOM  0.069997     0.022606   \n",
       "\n",
       "      energy_entropy_mean  spectral_centroid_mean  spectral_spread_mean  \\\n",
       "782              2.945650                0.191851              0.201698   \n",
       "615              2.967563                0.240208              0.213694   \n",
       "1405             2.918938                0.194020              0.206082   \n",
       "3800             2.968660                0.228308              0.200131   \n",
       "3412             3.006345                0.160496              0.182482   \n",
       "\n",
       "      spectral_entropy_mean  ...  delta chroma_4_std  delta chroma_5_std  \\\n",
       "782                0.689319  ...            0.019953            0.032110   \n",
       "615                0.823183  ...            0.020356            0.029753   \n",
       "1405               0.339042  ...            0.001415            0.022066   \n",
       "3800               0.996025  ...            0.017681            0.013259   \n",
       "3412               0.408786  ...            0.035430            0.013289   \n",
       "\n",
       "      delta chroma_6_std  delta chroma_7_std  delta chroma_8_std  \\\n",
       "782             0.032458            0.031829            0.003198   \n",
       "615             0.027887            0.016530            0.024693   \n",
       "1405            0.056758            0.029304            0.000626   \n",
       "3800            0.016366            0.011028            0.005618   \n",
       "3412            0.015212            0.015347            0.005723   \n",
       "\n",
       "      delta chroma_9_std  delta chroma_10_std  delta chroma_11_std  \\\n",
       "782             0.013863             0.024027             0.018933   \n",
       "615             0.022607             0.006172             0.008348   \n",
       "1405            0.033748             0.030512             0.029402   \n",
       "3800            0.003442             0.012716             0.022112   \n",
       "3412            0.018372             0.016502             0.024111   \n",
       "\n",
       "      delta chroma_12_std  delta chroma_std_std  \n",
       "782              0.010913              0.011224  \n",
       "615              0.004689              0.009163  \n",
       "1405             0.022541              0.014442  \n",
       "3800             0.025750              0.009388  \n",
       "3412             0.014215              0.010749  \n",
       "\n",
       "[5 rows x 140 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c38ae0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NEU    0.178718\n",
       "HAP    0.164359\n",
       "FEA    0.164359\n",
       "ANG    0.164359\n",
       "SAD    0.164103\n",
       "DIS    0.164103\n",
       "Name: Emotion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the percentages of the different emotion categories in the training set\n",
    "\n",
    "\n",
    "\n",
    "data_train.Emotion.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2eedfff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NEU    0.179303\n",
       "DIS    0.164959\n",
       "ANG    0.163934\n",
       "SAD    0.163934\n",
       "FEA    0.163934\n",
       "HAP    0.163934\n",
       "Name: Emotion, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the percentages of the different emotion categories in the test set\n",
    "\n",
    "data_test.Emotion.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b917cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the train and test(validataion) set based on the sentences \n",
    "\n",
    "# IEO - It's 11 o'clock.\n",
    "# TIE - That is exactly what happend.\n",
    "# IOM - I'm on my way to the meeting.\n",
    "# IWW - I wonder what this is about.\n",
    "# TAI - The airplane is almost full.\n",
    "# MTI - Maybe tomorrow it will be cold.\n",
    "# IWL - I would like a new alarm clock.\n",
    "# ITH - I think I have a doctor's appointment.\n",
    "# DFA - Dont forget a jacket.\n",
    "# ITS - I think I've seen this before.\n",
    "# TSI - The surface is slick.\n",
    "# WSI - We'll stop in a couple of minutes.\n",
    "\n",
    "# Get the rows of X_train, X_test corresponding to specific sentences\n",
    "\n",
    "# Split the train and test set into labels (y) and features (X)\n",
    "\n",
    "y_train = data_train[['Emotion']]\n",
    "y_test  = data_test[['Emotion']]\n",
    "\n",
    "X_train = data_train.drop(columns  = ['actorID', 'Emotion', 'SentenceID', 'Age', 'Race', 'Ethnicity'])\n",
    "X_test  = data_test.drop(columns   = ['actorID', 'Emotion', 'SentenceID', 'Age', 'Race', 'Ethnicity'])\n",
    "\n",
    "\n",
    "#X_train = data_train.drop(columns  = ['ActorID', 'Emotion', 'SentenceID'])\n",
    "#X_test  = data_test.drop(columns   = ['ActorID', 'Emotion', 'SentenceID'])\n",
    "\n",
    "\n",
    "X_train = data_train[['FileID','energy_mean','Sex']]\n",
    "X_test  = data_test[['FileID','energy_mean','Sex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the train and test(validataion) set based on the sentences \n",
    "\n",
    "# IEO - It's 11 o'clock.\n",
    "# TIE - That is exactly what happend.\n",
    "# IOM - I'm on my way to the meeting.\n",
    "# IWW - I wonder what this is about.\n",
    "# TAI - The airplane is almost full.\n",
    "# MTI - Maybe tomorrow it will be cold.\n",
    "# IWL - I would like a new alarm clock.\n",
    "# ITH - I think I have a doctor's appointment.\n",
    "# DFA - Dont forget a jacket.\n",
    "# ITS - I think I've seen this before.\n",
    "# TSI - The surface is slick.\n",
    "# WSI - We'll stop in a couple of minutes.\n",
    "\n",
    "# Get the rows of X_train, X_test corresponding to specific sentences\n",
    "\n",
    "# Split the train and test set into labels (y) and features (X)\n",
    "\n",
    "y_train = data_train[['Emotion']]\n",
    "y_test  = data_test[['Emotion']]\n",
    "\n",
    "X_train = data_train.drop(columns  = ['actorID', 'Emotion', 'SentenceID', 'Age', 'Race', 'Ethnicity'])\n",
    "X_test  = data_test.drop(columns   = ['actorID', 'Emotion', 'SentenceID', 'Age', 'Race', 'Ethnicity'])\n",
    "\n",
    "\n",
    "#X_train = data_train.drop(columns  = ['ActorID', 'Emotion', 'SentenceID'])\n",
    "#X_test  = data_test.drop(columns   = ['ActorID', 'Emotion', 'SentenceID'])\n",
    "\n",
    "\n",
    "X_train = data_train[['FileID','energy_mean','Sex']]\n",
    "X_test  = data_test[['FileID','energy_mean','Sex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the train and test(validataion) set based on the sentences \n",
    "\n",
    "# IEO - It's 11 o'clock.\n",
    "# TIE - That is exactly what happend.\n",
    "# IOM - I'm on my way to the meeting.\n",
    "# IWW - I wonder what this is about.\n",
    "# TAI - The airplane is almost full.\n",
    "# MTI - Maybe tomorrow it will be cold.\n",
    "# IWL - I would like a new alarm clock.\n",
    "# ITH - I think I have a doctor's appointment.\n",
    "# DFA - Dont forget a jacket.\n",
    "# ITS - I think I've seen this before.\n",
    "# TSI - The surface is slick.\n",
    "# WSI - We'll stop in a couple of minutes.\n",
    "\n",
    "\n",
    "X_train.FileID = X_train[\"FileID\"].str[5:8]\n",
    "X_test.FileID  = X_test[\"FileID\"].str[5:8]\n",
    "\n",
    "################################################\n",
    "#X_train_sen = X_train.loc[(X_train.FileID == 'IOE') | (X_train.FileID == 'TIE') | (X_train.FileID == 'MTI') | (X_train.FileID == 'DFA')]\n",
    "#X_test_sen  = X_test.loc[(X_test.FileID == 'IOE') | (X_test.FileID == 'TIE') | (X_test.FileID == 'MTI') | (X_train.FileID == 'DFA')]\n",
    "\n",
    "#y_train_sen = y_train.loc[(X_train.FileID == 'IOE') | (X_train.FileID == 'TIE') | (X_train.FileID == 'MTI') | (X_train.FileID == 'DFA')]\n",
    "#y_test_sen  = y_test.loc[(X_test.FileID == 'IOE') | (X_test.FileID == 'TIE') | (X_test.FileID == 'MTI') | (X_train.FileID == 'DFA')]\n",
    "\n",
    "#################################################\n",
    "#X_train_sen = X_train.loc[(X_train.FileID == 'IWW') | (X_train.FileID == 'ITH') | (X_train.FileID == 'ITS')]\n",
    "#X_test_sen  = X_test.loc[(X_test.FileID == 'IWW') | (X_test.FileID == 'ITH') | (X_test.FileID == 'ITS')]\n",
    "\n",
    "#y_train_sen = y_train.loc[(X_train.FileID == 'IWW') | (X_train.FileID == 'ITH') | (X_train.FileID == 'ITS')]\n",
    "#y_test_sen  = y_test.loc[(X_test.FileID == 'IWW') | (X_test.FileID == 'ITH') | (X_test.FileID == 'ITS')]\n",
    "\n",
    "#################################################\n",
    "#X_train_sen = X_train.loc[(X_train.FileID == 'TAI') | (X_train.FileID == 'IWL') | (X_train.FileID == 'IOM') | (X_train.FileID == 'TSI') | (X_train.FileID == 'WSI')]\n",
    "#X_test_sen  = X_test.loc[(X_test.FileID == 'TAI') | (X_test.FileID == 'IWL') | (X_test.FileID == 'IOM') | (X_train.FileID == 'TSI') | (X_train.FileID == 'WSI')]\n",
    "\n",
    "#y_train_sen = y_train.loc[(X_train.FileID == 'TAI') | (X_train.FileID == 'IWL') | (X_train.FileID == 'IOM') | (X_train.FileID == 'TSI') | (X_train.FileID == 'WSI')]\n",
    "#y_test_sen  = y_test.loc[(X_test.FileID == 'TAI') | (X_test.FileID == 'IWL') | (X_test.FileID == 'IOM') | (X_train.FileID == 'TSI') | (X_train.FileID == 'WSI')]\n",
    "\n",
    "##################################################\n",
    "\n",
    "#X_train_sen1 = X_train.loc[(X_train.FileID == 'IWW') | (X_train.FileID == 'ITH') | (X_train.FileID == 'ITS')]\n",
    "#X_test_sen1  = X_test.loc[(X_test.FileID == 'IWW') | (X_test.FileID == 'ITH') | (X_test.FileID == 'ITS')]\n",
    "\n",
    "#y_train_sen1 = y_train.loc[(X_train.FileID == 'IWW') | (X_train.FileID == 'ITH') | (X_train.FileID == 'ITS')]\n",
    "#y_test_sen1  = y_test.loc[(X_test.FileID == 'IWW') | (X_test.FileID == 'ITH') | (X_test.FileID == 'ITS')]\n",
    "\n",
    "#X_train_sen = X_train_sen.loc[(X_train.Sex == 'Male')]\n",
    "#X_test_sen  = X_test_sen.loc[(X_test.Sex == 'Male')]\n",
    "\n",
    "#y_train_sen = y_train_sen.loc[(X_train.Sex == 'Male')]\n",
    "#y_test_sen  = y_test_sen.loc[(X_test.Sex == 'Male')]\n",
    "\n",
    "\n",
    "\n",
    "X_train_sen1 = X_train\n",
    "X_test_sen1  = X_test\n",
    "\n",
    "y_train_sen1 = y_train\n",
    "y_test_sen1 = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5857ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sen = X_train_sen1.loc[(X_train.Sex == 'Male')]\n",
    "X_test_sen  = X_test_sen1.loc[(X_test.Sex == 'Male')]\n",
    "\n",
    "y_train_sen = y_train_sen1.loc[(X_train.Sex == 'Male')]\n",
    "y_test_sen  = y_test_sen1.loc[(X_test.Sex == 'Male')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f50f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19680db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sen = X_train_sen.drop(columns = ['FileID','Sex'])\n",
    "X_test_sen  = X_test_sen.drop(columns = ['FileID','Sex'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1c94d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a column to the y vectors encoding each of the emotions.\n",
    "\n",
    "y_train_sen_dummies = pd.get_dummies(y_train_sen)\n",
    "y_train_sen         = pd.concat([y_train_sen, y_train_sen_dummies], axis=1)\n",
    "\n",
    "y_test_sen_dummies  = pd.get_dummies(y_test)\n",
    "y_test_sen          = pd.concat([y_test_sen, y_test_sen_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2275cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the encoding looks right\n",
    "\n",
    "X_train_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sen.Emotion[y_train_sen.Emotion == 'NEU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: try to train a support vector machine with Gaussian radial kernel to distinguish between instances\n",
    "# where emotion is 'ANG' and instances where emotion is 'HAP'. \n",
    "\n",
    "# Get the rows of X_train, X_test corresponding to just the emotions 'ANG' and 'HAP'\n",
    "\n",
    "X_train_sub = X_train_sen.loc[(y_train_sen.Emotion == 'HAP') | (y_train_sen.Emotion == 'NEU')]\n",
    "X_test_sub  = X_test_sen.loc[(y_test_sen.Emotion == 'HAP') | (y_test_sen.Emotion == 'NEU')]\n",
    "\n",
    "\n",
    "# Get the Emotion_ANG column of the ys, with only the rows corresponding to 'ANG' and 'HAP'\n",
    "\n",
    "y_train_sub = y_train_sen.loc[(y_train_sen.Emotion == 'HAP') | (y_train_sen.Emotion == 'NEU')].Emotion_HAP\n",
    "y_test_sub = y_test_sen.loc[(y_test_sen.Emotion == 'HAP') | (y_test_sen.Emotion == 'NEU')].Emotion_HAP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79e1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_sub\",  X_train_sub.shape)\n",
    "print(\"y_train_sub\",  y_train_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c46759",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline to first scale the mid feature data, then apply the SVC\n",
    "\n",
    "pipe = Pipeline([('scale', StandardScaler()),\n",
    "                 ('svc', SVC(kernel='rbf'))])\n",
    "\n",
    "\n",
    "# Fit the model to the training data\n",
    "\n",
    "pipe.fit(X_train_sub, y_train_sub)\n",
    "\n",
    "    \n",
    "# Get the model's prediction on the test data\n",
    "\n",
    "pred = pipe.predict(X_test_sub)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d707f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline to first scale the mid feature data, then apply the SVC\n",
    "\n",
    "pipe = Pipeline([('scale', StandardScaler()),\n",
    "                 ('svc', SVC(kernel='rbf'))])\n",
    "\n",
    "\n",
    "# Fit the model t#o the training data\n",
    "\n",
    "pipe.fit(X_train_sub, y_train_sub)\n",
    "\n",
    "    \n",
    "# Get the model's prediction on the test data\n",
    "\n",
    "pred = pipe.predict(X_test_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad669cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the confusion matrix for the test data :\n",
    "\n",
    "print(\"confusion matrix for the test set is:\")\n",
    "print(confusion_matrix(y_test_sub, pred))\n",
    "print()\n",
    "\n",
    "# Look at the confusion matrix for the training data:\n",
    "pred_train = pipe.predict(X_train_sub)\n",
    "print(\"confusion matrix for the train set is:\")\n",
    "print(confusion_matrix(y_train_sub, pred_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f04ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d7866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b427d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
