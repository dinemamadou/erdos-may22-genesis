{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d12fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a dark background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9fecfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in your csv file that has the mid features. \n",
    "\n",
    "#data = pd.read_csv('../Feature Extraction/midFeaturesTrainSet.csv')\n",
    "data = pd.read_csv('midFeaturesTrainFinal.csv')\n",
    "\n",
    "\n",
    "data.head()\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test set, stratified by Emotion.\n",
    "\n",
    "\n",
    "data_train, data_test = train_test_split(data.copy(),\n",
    "                                   shuffle=True,\n",
    "                                   random_state=608,\n",
    "                                   stratify=data.Emotion,\n",
    "                                   test_size=0.2\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e4f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ae0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the percentages of the different emotion categories in the training set\n",
    "\n",
    "data_train.Emotion.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eedfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the percentages of the different emotion categories in the test set\n",
    "\n",
    "data_test.Emotion.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b917cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[['Emotion']]\n",
    "y_test  = data_test[['Emotion']]\n",
    "\n",
    "X_train = data_train.drop(columns  = ['FileID','actorID', 'Emotion', 'SentenceID'])\n",
    "X_test  = data_test.drop(columns   = ['FileID','actorID', 'Emotion', 'SentenceID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8adda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1c94d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a column to the y vectors encoding each of the emotions.\n",
    "\n",
    "y_train_dummies = pd.get_dummies(y_train)\n",
    "y_train         = pd.concat([y_train, y_train_dummies], axis=1)\n",
    "\n",
    "y_test_dummies  = pd.get_dummies(y_test)\n",
    "y_test          = pd.concat([y_test, y_test_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: try to train a support vector machine with Gaussian radial kernel to distinguish between instances\n",
    "# where emotion is 'ANG' and instances where emotion is 'HAP'. \n",
    "\n",
    "# Get the rows of X_train, X_test corresponding to just the emotions 'ANG' and 'HAP'\n",
    "\n",
    "X_train_sub = X_train.loc[(y_train.Emotion == 'DIS') | (y_train.Emotion == 'NEU')]\n",
    "X_test_sub  = X_test.loc[(y_test.Emotion == 'DIS') | (y_test.Emotion == 'NEU')]\n",
    "\n",
    "\n",
    "# Get the Emotion_ANG column of the ys, with only the rows corresponding to 'ANG' and 'HAP'\n",
    "\n",
    "y_train_sub = y_train.loc[(y_train.Emotion == 'DIS') | (y_train.Emotion == 'NEU')].Emotion_DIS\n",
    "y_test_sub  = y_test.loc[(y_test.Emotion == 'DIS') | (y_test.Emotion == 'NEU')].Emotion_DIS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879de331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_sub = X_train\n",
    "#X_test_sub  = X_test\n",
    "\n",
    "#y_train_sub = y_train\n",
    "#y_test_sub  = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79e1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_sub\",  X_train_sub.shape)\n",
    "print(\"y_train_sub\",  y_train_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c46759",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline to first scale the mid feature data, then apply the SVC\n",
    "\n",
    "pipe = Pipeline([('scale', StandardScaler()),\n",
    "                 ('svc', SVC(kernel='rbf'))])\n",
    "\n",
    "\n",
    "# Fit the model to the training data\n",
    "\n",
    "pipe.fit(X_train_sub, y_train_sub)\n",
    "\n",
    "    \n",
    "# Get the model's prediction on the test data\n",
    "\n",
    "pred = pipe.predict(X_test_sub)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d707f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline to first scale the mid feature data, then apply the SVC\n",
    "\n",
    "pipe = Pipeline([('scale', StandardScaler()),\n",
    "                 ('svc', SVC(kernel='rbf'))])\n",
    "\n",
    "\n",
    "# Fit the model t#o the training data\n",
    "\n",
    "pipe.fit(X_train_sub, y_train_sub)\n",
    "\n",
    "    \n",
    "# Get the model's prediction on the test data\n",
    "\n",
    "pred = pipe.predict(X_test_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad669cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the confusion matrix for the test data :\n",
    "\n",
    "print(\"confusion matrix for the test set is:\")\n",
    "print(confusion_matrix(y_test_sub, pred))\n",
    "print()\n",
    "\n",
    "# Look at the confusion matrix for the training data:\n",
    "pred_train = pipe.predict(X_train_sub)\n",
    "print(\"confusion matrix for the train set is:\")\n",
    "print(confusion_matrix(y_train_sub, pred_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f04ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d7866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b427d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
